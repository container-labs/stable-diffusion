version: "3"

volumes:
  pip_cache:
  model_cache:
  conda_cache:

services:
  app:
    # force amd so it's the same as remote
    platform: linux/amd64
    build:
      context: ./docker/notebook
      dockerfile: Dockerfile
    # command: python3 main.py
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - XRT_TPU_CONFIG="localservice;0;localhost:51011"
    # mps not supported yet
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    # https://stackoverflow.com/questions/43844639/how-do-i-add-cached-or-delegated-into-a-docker-compose-yml-volumes-list
    volumes:
      - pip_cache:/root/.cache/pip:delegated
      # TODO: make this one sync w/ localhost
      - model_cache:/root/.cache/huggingface:delegated
      - ./image_dir:/image_dir:default

  base:
    platform: linux/amd64
    build:
      context: ./ml-base
      dockerfile: Dockerfile.base
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - pip_cache:/root/.cache/pip:delegated
      # TODO: make this one sync w/ localhost
      - model_cache:/root/.cache/huggingface:delegated
      - ./image_dir:/image_dir:default
  training-base:
    platform: linux/amd64
    build:
      context: ./ml-base
      dockerfile: Dockerfile.training-base
      args:
        - BASE_IMAGE=stable-diffusion_base
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - conda_cache:/root/.conda:delegated
      - pip_cache:/root/.cache/pip:delegated
      # TODO: make this one sync w/ localhost
      - model_cache:/root/.cache/huggingface:delegated
      - ./image_dir:/image_dir:default

  training:
    # force amd so it's the same as remote
    platform: linux/amd64
    build:
      context: ./docker/training
      dockerfile: Dockerfile
      args:
        - BASE_IMAGE=stable-diffusion_training-base
      # entrypoint:
      #   - ./train-style.sh
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - conda_cache:/root/.conda:delegated
      - pip_cache:/root/.cache/pip:delegated
      - ./model-cache:/root/.cache/huggingface:default
      - ./image_dir:/image_dir:default

  orchestration:
    build:
      context: ./py-train
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - GOOGLE_APPLICATION_CREDENTIALS=creds.json
      - GOOGLE_CLOUD_PROJECT=md-wbeebe-0808-example-apps
      - REGION=us-central1
      - GCS_BUCKET=gs://md-ml
      - GCP_TOPIC=training-requests
      - GCP_SUBSCRIPTION=trainer-sub

  model-loader:
    build:
      context: ./docker/model-loader
      dockerfile: Dockerfile
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - GOOGLE_APPLICATION_CREDENTIALS=creds.json
      - PROJECT_ID=md-wbeebe-0808-example-apps
      - REGION=us-central1
      - GCS_BUCKET=gs://md-ml

  gcloud:
    image: google/cloud-sdk:latest
    volumes:
      - ./creds.json:/creds.json
      - ./gcloud-vol:/root/gcloud-vol
    command: >
      bash -c "
      mkdir -p /root/gcloud-vol/job-1668365875 &&
      gcloud auth activate-service-account --key-file=/creds.json &&
      gcloud config set project md-wbeebe-0808-example-apps &&
      gsutil ls gs://md-ml &&
      gsutil -m cp -r \
        "gs://md-ml/job-1668365875/feature_extractor" \
        "gs://md-ml/job-1668365875/learned_embeds.bin" \
        "gs://md-ml/job-1668365875/model_index.json" \
        "gs://md-ml/job-1668365875/safety_checker" \
        "gs://md-ml/job-1668365875/scheduler" \
        "gs://md-ml/job-1668365875/text_encoder" \
        "gs://md-ml/job-1668365875/tokenizer" \
        "gs://md-ml/job-1668365875/unet" \
        "gs://md-ml/job-1668365875/vae" \
        /root/gcloud-vol/job-1668365875
      "
